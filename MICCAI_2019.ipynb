{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7L7_DBmQzrY_"
   },
   "source": [
    "# Using a NiftyNet neural network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bsZ0Xi2z2Je"
   },
   "source": [
    "*Image segmentation using deep learning* were the 5 most common words in all full paper titles from **both** [MICCAI 2018](https://www.miccai2018.org/en/) and [MIDL 2019](https://2019.midl.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">&quot;Image segmentation using deep learning&quot;, guess this is the hottest topic in MIDL <a href=\"https://twitter.com/hashtag/MIDL2019?src=hash&amp;ref_src=twsrc%5Etfw\">#MIDL2019</a> <a href=\"https://twitter.com/midl_conference?ref_src=twsrc%5Etfw\">@midl_conference</a> <a href=\"https://t.co/64smdMjnxY\">pic.twitter.com/64smdMjnxY</a></p>&mdash; Hua Ma (@forever_pippo) <a href=\"https://twitter.com/forever_pippo/status/1148329951550197760?ref_src=twsrc%5Etfw\">July 8, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Just like <a href=\"https://twitter.com/hashtag/miccai2018?src=hash&amp;ref_src=twsrc%5Etfw\">#miccai2018</a>! <a href=\"https://t.co/3ZTHxj9iPT\">pic.twitter.com/3ZTHxj9iPT</a></p>&mdash; Julia Schnabel (@ja_schnabel) <a href=\"https://twitter.com/ja_schnabel/status/1148356705916526592?ref_src=twsrc%5Etfw\">July 8, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably due to the great success that convolutional neural networks (CNNs) have achieved in this field in the past years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives some low-level intuition about parameters stored in a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NiftyNet\n",
    "<a href=\"https://niftynet.io/\"><img src=\"https://niftynet.io/img/niftynet-logo.png\" alt=\"drawing\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NiftyNet](https://niftynet.io/) is \"an open source convolutional neural networks platform for medical image analysis and image-guided therapy\" built on top of [TensorFlow](https://www.tensorflow.org/). It is probably the easiest way to get started with deep learning for medical image and is mostly been used for segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owZ4Cew-0D-H"
   },
   "source": [
    "## HighRes3DNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HighRes3DNet is a residual convolutional neural network designed to have a large receptive field and preserve a high resolution using a relatively small number of parameters. It was presented in 2017 by Li et al. at IPMI: [*On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task*](https://arxiv.org/abs/1707.01992)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fepegar/tf2pt/raw/master/images/network.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors used [NiftyNet](https://niftynet.io/) to train a model based on this architecture to perform [brain parcellation](https://ieeexplore.ieee.org/document/7086081?arnumber=7086081) from $T_1$-weighted MR images using the [ADNI dataset](http://adni.loni.usc.edu/).\n",
    "\n",
    "This is an qualitative result the paper:\n",
    "\n",
    "<img src=\"https://github.com/fepegar/tf2pt/raw/master/images/li-000.png\" alt=\"drawing\" width=\"150\"/> <img src=\"https://github.com/fepegar/tf2pt/raw/master/images/li-001.png\" alt=\"drawing\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the architecture is on [NiftyNet's GitHub repository](https://github.com/NifTK/NiftyNet/blob/dev/niftynet/network/highres3dnet.py) and the authors have have uploaded thre parameters and configuration file to the [Model Zoo](https://github.com/NifTK/NiftyNetModelZoo/tree/5-reorganising-with-lfs/highres3dnet_brain_parcellation). After reading the paper and the code, it is relatively straightforward to implement the architecture using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltcwoYBm7_fF"
   },
   "source": [
    "In this notebook we will:\n",
    "\n",
    "1. [Extract the parameters from a TensorFlow checkpoint](#TensorFlow-world)\n",
    "2. [Transform them to PyTorch](#PyTorch-world)\n",
    "3. [Apply the model to some test data](#Run-inference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZzLgIR80mVQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BIsvPiS4ghW"
   },
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "X2uoosvx1uxh",
    "outputId": "7c0565e4-7558-461b-eedd-8805d08784fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.10.0 (from -r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/64/ca/830b7cedb073ae264d215d51bd18d7cff7a2a47e39d79f6fa23edae17bb2/tensorflow_gpu-1.10.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting niftynet (from -r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/7d/f8b8f28e9872f6b00e244007a5c789c7db30cc904bbc0c150a4bfe4d2cfb/NiftyNet-0.5.0-py2.py3-none-any.whl\n",
      "Collecting scikit-image (from -r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/ab/674e168bf7d0bc597218b3bec858d02c23fbac9ec1fec9cad878c6cee95f/scikit_image-0.15.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting matplotlib (from -r requirements.txt (line 4))\n",
      "  Using cached https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pandas (from -r requirements.txt (line 5))\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/9a/7eb9952f4b4d73fbd75ad1d5d6112f407e695957444cb695cbb3cdab918a/pandas-0.25.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting nibabel (from -r requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/66/30/fbed62172920c3fd050b6483541546a87c5e735f4a0ef03f08bb150680b4/nibabel-2.4.1-py2.py3-none-any.whl\n",
      "Collecting torch (from -r requirements.txt (line 7))\n",
      "  Using cached https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting torchsummary (from -r requirements.txt (line 8))\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\n",
      "Collecting highresnet (from -r requirements.txt (line 9))\n",
      "Collecting seaborn (from -r requirements.txt (line 10))\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.11.0,>=1.10.0 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "Collecting gast>=0.2.0 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "Collecting grpcio>=1.8.6 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/5d/b434403adb2db8853a97828d3d19f2032e79d630e0d11a8e95d243103a11/grpcio-1.22.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting protobuf>=3.6.0 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/0e/e7cdff89745986c984ba58e6ff6541bc5c388dd9ab9d7d312b3b1532584a/protobuf-3.9.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "Requirement already satisfied: six>=1.10.0 in /home/fernando/miniconda3/envs/emiccai/lib/python3.6/site-packages (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1)) (1.12.0)\n",
      "Collecting numpy<=1.14.5,>=1.13.3 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting setuptools<=39.1.0 (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in /home/fernando/miniconda3/envs/emiccai/lib/python3.6/site-packages (from tensorflow-gpu==1.10.0->-r requirements.txt (line 1)) (0.33.4)\n",
      "Collecting packaging (from niftynet->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/91/32/58bc30e646e55eab8b21abf89e353f59c0cc02c417e42929f4a9546e1b1d/packaging-19.0-py2.py3-none-any.whl\n",
      "Collecting blinker (from niftynet->-r requirements.txt (line 2))\n",
      "Collecting pillow (from niftynet->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/14/41/db6dec65ddbc176a59b89485e8cc136a433ed9c6397b6bfe2cd38412051e/Pillow-6.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting scipy>=0.18 (from niftynet->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting configparser (from niftynet->-r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/ba/05/6c96328e92e625fc31445d24d75a2c92ef9ba34fc5b037fe69693c362a0d/configparser-3.7.4-py2.py3-none-any.whl\n",
      "Collecting networkx>=2.0 (from scikit-image->-r requirements.txt (line 3))\n",
      "Collecting imageio>=2.0.1 (from scikit-image->-r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/af/0a/943c965d372dae0b1f1482677d29030ab834351a61a9a632fd62f27f1523/imageio-2.5.0-py3-none-any.whl\n",
      "Collecting PyWavelets>=0.4.0 (from scikit-image->-r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/4e/cd/528dba0b474b08f6f9a3a5e1b4bb23d8e33ed5d9f0e321cc967c2607df05/PyWavelets-1.0.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Using cached https://files.pythonhosted.org/packages/ff/2c/afc36631a9e0dc9f2b8bd4c19fa2d330cbe07d6679a1c0910418a200acad/pyparsing-2.4.1.1-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 4))\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/fernando/miniconda3/envs/emiccai/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.0)\n",
      "Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 5))\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.11.0,>=1.10.0->tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/ab/d3bed6b92042622d24decc7aadc8877badf18aeca1571045840ad4956d3f/Werkzeug-0.15.5-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.11.0,>=1.10.0->tensorflow-gpu==1.10.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/fernando/miniconda3/envs/emiccai/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 3)) (4.4.0)\n",
      "Installing collected packages: astor, numpy, werkzeug, setuptools, protobuf, markdown, tensorboard, absl-py, gast, grpcio, termcolor, tensorflow-gpu, pyparsing, packaging, blinker, nibabel, pillow, scipy, configparser, pytz, pandas, niftynet, networkx, imageio, PyWavelets, kiwisolver, cycler, matplotlib, scikit-image, torch, torchsummary, highresnet, seaborn\n",
      "  Found existing installation: setuptools 41.0.1\n",
      "    Uninstalling setuptools-41.0.1:\n",
      "      Successfully uninstalled setuptools-41.0.1\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.platform.tf_logging import _get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "colab_type": "code",
    "id": "TdjKHmBb82qG",
    "outputId": "0be16fb6-54ad-4a94-d513-2886c27fa22d"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "from niftynet.io.image_reader import ImageReader\n",
    "from niftynet.engine.sampler_grid_v2 import GridSampler\n",
    "from niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\n",
    "from niftynet.layer.pad import PadLayer\n",
    "from niftynet.layer.binary_masking import BinaryMaskingLayer\n",
    "from niftynet.layer.histogram_normalisation import HistogramNormalisationLayer\n",
    "from niftynet.layer.mean_variance_normalisation import MeanVarNormalisationLayer\n",
    "\n",
    "from highresnet import HighRes3DNet\n",
    "\n",
    "import tf2pt\n",
    "import utils\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6Z5ftE48fxv"
   },
   "source": [
    "### Download NiftyNet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [NiftyNet's `net_download`](https://github.com/NifTK/NiftyNetModelZoo/tree/5-reorganising-with-lfs/highres3dnet_brain_parcellation#downloading-model-zoo-files) to get all we need from the [Model Zoo](https://github.com/NifTK/NiftyNetModelZoo/tree/5-reorganising-with-lfs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fA92bog98ewS"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!net_download highres3dnet_brain_parcellation_model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niftynet_dir = Path('~/niftynet').expanduser()\n",
    "utils.list_files(niftynet_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three directories under `~/niftynet`:\n",
    "1. `extensions` is a Python package and contains the [configuration file](https://niftynet.readthedocs.io/en/dev/config_spec.html)\n",
    "2. `models` contains the landmarks for [histogram standardization](https://ieeexplore.ieee.org/document/836373) and the network parameters\n",
    "3. `data` contains an [OASIS](https://www.oasis-brains.org/) MRI that can be used to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3Ll7LVl4WtC"
   },
   "outputs": [],
   "source": [
    "models_dir = niftynet_dir / 'models'\n",
    "zoo_entry = 'highres3dnet_brain_parcellation'\n",
    "checkpoint_name = 'model.ckpt-33000'\n",
    "checkpoint_path = models_dir / zoo_entry / 'models' / checkpoint_name\n",
    "data_dir = niftynet_dir / 'data' / 'OASIS'\n",
    "config_path = niftynet_dir / 'extensions' / zoo_entry / 'highres3dnet_config_eval.ini'\n",
    "histogram_landmarks_path = models_dir / zoo_entry / 'databrain_std_hist_models_otsu.txt'\n",
    "tempdir = Path(tempfile.gettempdir()) / 'miccai_niftynet_pytorch'\n",
    "tempdir.mkdir(exist_ok=True)\n",
    "csv_tf_path = tempdir / 'variables_tf.csv'\n",
    "state_dict_tf_path = tempdir / 'state_dict_tf.pth'\n",
    "state_dict_pt_path = tempdir / 'state_dict_pt.pth'\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)  # do not truncate strings when displaying data frames\n",
    "pd.set_option('display.max_rows', None)    # show all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the path to the checkpoint is not a real filepath but the basename of the three checkpoint files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svG_jtqn0KGU"
   },
   "source": [
    "### TensorFlow world \n",
    "<a href=\"https://www.tensorflow.org/\"><img src=\"https://static.nvidiagrid.net/ngc/containers/tensorflow.png\" alt=\"drawing\" width=\"50\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what variables are stored in the checkpoint.\n",
    "\n",
    "Some of them are filtered out by `tf2pt.checkpoint_to_state_dict()` for clarity:\n",
    "* Variables used by the Adam optimizer during training\n",
    "* Variables with no shape. They won't help much\n",
    "* Variables containing `biased` or `ExponentialMovingAverage`. Results using these variables have turned out to be different to the ones produced by NiftyNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store the variables names in a data frame to list them in this notebook and the values in a Python dictionary to retrieve them later.\n",
    "\n",
    "I figured out the code in `tf2pt.checkpoint_to_state_dict` reading the corresponding [TensorFlow docs](https://www.tensorflow.org/api_docs/python/tf/train/list_variables) and [Stack Overflow answers](https://stackoverflow.com/search?q=restore+tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2pt.checkpoint_to_state_dict(checkpoint_path, csv_tf_path, state_dict_tf_path)\n",
    "data_frame_tf = pd.read_csv(csv_tf_path)\n",
    "state_dict_tf = torch.load(state_dict_tf_path)\n",
    "data_frame_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers names and parameters shapes overall seem to be coherent with the figure in the paper, but there's an additional $1 \\times 1 \\times 1$ convolutional layer with 80 output channels. It's also in the [code](https://github.com/NifTK/NiftyNet/blob/1832a516c909b67d0d9618acbd04a7642c12efca/niftynet/network/highres3dnet.py#L93). It seems to be the model with [dropout](http://jmlr.org/papers/v15/srivastava14a.html) used in the study to compute the model's uncertainty, so [our implementation of the architecture](https://github.com/fepegar/highresnet/blob/f434266a51924681f95b01a0f03611fbf1148db6/highresnet/highresnet.py#L82-L97) should include this layer as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three [dilation](https://arxiv.org/abs/1511.07122) blocks composed of three [residual](https://arxiv.org/abs/1512.03385) blocks each, which have two convolutional layers inside. That's $3 \\times 3 \\times 2 = 18$ layers. The other three layers are the initial convolution before the first residual block, the conv layer before the dropout and the classifier at the end. Apparently, *all* the convolutional layers have an associated [batch normalization]((https://arxiv.org/abs/1502.03167)) layer. That makes 21 convolutional layers and 21 batch normalization layers, all with parameters that must be transferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the weights of each conovolutional kernel represents the three spatial dimensions, the input channels and the output channels: $(D, H, W, C_{in}, C_{out})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch norm layer contains 4 parameter groups. Moving mean and variance of the batches, and the affine transformation parameters $\\gamma$ (scale or *weight*) and $\\beta$ (shift or *bias*):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the total number of parameter groups is $21 + 21 \\times 4 = 105$. The convolutional layers don't use a bias parameter, as it's not necessary when using batch norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch world\n",
    "<a href=\"https://pytorch.org/\"><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/01/pytorch-logo.png\" alt=\"drawing\" width=\"100\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_modalities = 1\n",
    "num_classes = 160\n",
    "model = HighRes3DNet(num_input_modalities, num_classes, add_dropout_layer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the variable names created by PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_pt = model.state_dict()\n",
    "rows = []\n",
    "for name, parameters in state_dict_pt.items():\n",
    "    if 'num_batches_tracked' in name:  # filter out for clarity\n",
    "        continue\n",
    "    shape = ', '.join(str(n) for n in parameters.shape)\n",
    "    row = {'name': name, 'shape': shape}\n",
    "    rows.append(row)\n",
    "df_pt = pd.DataFrame.from_dict(rows)\n",
    "df_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in Pytorch, `moving_mean` and `moving_variance` are `running_mean` and `running_var`. Also, $\\gamma$ and $\\beta$ are called `weight` and `bias`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional kernels have a different arrangement: $(C_{out}, C_{in}, D, H, W)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names and shapes look coherent and there are 105 lines in both lists, so we should be able to create a mapping between the TensorFlow and the PyTorch variables. The function `tf2pt` receives a TensorFlow-like variable and returns the corresponding PyTorch-like variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_tf, tensor_tf in state_dict_tf.items():\n",
    "    shape_tf = tuple(tensor_tf.shape)\n",
    "    print(f'{str(shape_tf):18}', name_tf) \n",
    "    \n",
    "    # Convert TensorFlow name to PyTorch name\n",
    "    name_pt, tensor_pt = tf2pt.tf2pt(name_tf, tensor_tf)\n",
    "    \n",
    "    shape_pt = tuple(state_dict_pt[name_pt].shape)\n",
    "    print(f'{str(shape_pt):18}', name_pt)\n",
    "    \n",
    "    # Sanity check\n",
    "    if sum(shape_tf) != sum(shape_pt):\n",
    "        raise ValueError\n",
    "    \n",
    "    state_dict_pt[name_pt] = tensor_pt\n",
    "    print()\n",
    "torch.save(state_dict_pt, state_dict_pt_path)\n",
    "print('State dictionary saved to', state_dict_pt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If PyTorch is happy when loading our state dict into the model, we should be on the right track 🤞..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No incompatible keys. Yay! 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCOrhfw10W-H"
   },
   "source": [
    "### Plot weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something great about PyTorch is that the modules parameters are easily accesible. Let's plot some of them before and after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = model\n",
    "model_init = HighRes3DNet(num_input_modalities, num_classes, add_dropout_layer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By [default](https://github.com/pytorch/pytorch/blob/77353636de32a207cf0a332395f91011bc2f07fb/torch/nn/modules/conv.py#L48-L53), convolutional layers in PyTorch are initialized using [He uniform variance scaling](https://arxiv.org/abs/1502.01852):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_all_parameters(model_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what they look like after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_all_parameters(model_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to match the configuration used during training in order to obtain consistent results. This are the relevant contents of the downloaded [configuration file](https://niftynet.readthedocs.io/en/dev/config_spec.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ini\n",
    "[Modality0]\n",
    "path_to_search = data/OASIS/\n",
    "filename_contains = nii\n",
    "pixdim = (1.0, 1.0, 1.0)\n",
    "axcodes = (R, A, S)\n",
    "\n",
    "[NETWORK]\n",
    "name = highres3dnet\n",
    "volume_padding_size = 10\n",
    "whitening = True\n",
    "normalisation = True\n",
    "normalise_foreground_only=True\n",
    "foreground_type = mean_plus\n",
    "histogram_ref_file = databrain_std_hist_models_otsu.txt\n",
    "cutoff = (0.001, 0.999)\n",
    "\n",
    "[INFERENCE]\n",
    "border = 2\n",
    "spatial_window_size = (128, 128, 128)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser()\n",
    "config.read(config_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out the necessary preprocessing reading the paper, the [code](https://github.com/NifTK/NiftyNet/blob/61f2a8bbac1348591412c00f55d1c19b91c0367f/niftynet/application/segmentation_application.py#L95-L192) and the [configuration file](https://niftynet.readthedocs.io/en/dev/config_spec.html) that was downloaded. NiftyNet offers some powerful I/O tools. We will use readers, samplers and aggregators to read and write the necessary files. Some of the [demos](https://github.com/NifTK/NiftyNet/tree/dev/demos/module_examples) show how they can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = dict(\n",
    "    path_to_search=str(data_dir),\n",
    "    filename_contains='nii',\n",
    "    axcodes=('R', 'A', 'S'),\n",
    "    pixdim=(1, 1, 1),\n",
    ")\n",
    "data_parameters = {\n",
    "    'image': input_dict,\n",
    "}\n",
    "reader = ImageReader().initialise(data_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, image_data_dict, _ = reader()\n",
    "original_image = image_data_dict['image']\n",
    "original_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of our image and knowing it's now in [RAS+ orientation](http://www.grahamwideman.com/gw/brain/orientation/orientterms.htm), we can see that it represents $160$ sagittal slices of $256 \\times 256$ pixels, with $1$ channel (monomodal) and $1$ time point. Let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_volume(original_image, title='Original volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_histogram(original_image, kde=False, labels=True, ylim=(0, 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We [pad the input volume](https://niftynet.readthedocs.io/en/dev/window_sizes.html#volume-padding-size) and crop the output volume to reduce the border effect introduced by the padded convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_padding_layer = PadLayer(\n",
    "    image_name=['image'],  # https://github.com/NifTK/NiftyNet/blob/61f2a8bbac1348591412c00f55d1c19b91c0367f/niftynet/layer/pad.py#L52\n",
    "    border=(10, 10, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a masking function in order to use only the foreground voxels for normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_masking_func = BinaryMaskingLayer(type_str=config['NETWORK']['foreground_type'])\n",
    "mask = binary_masking_func(original_image)\n",
    "visualization.plot_volume(mask, enhance=False, title='Binary mask for preprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [MRI histogram standardization](https://ieeexplore.ieee.org/document/836373) trained on the training dataset for our test image. We use the mean intensity of the volume as a threshold for the mask, as the authors claim that this usually gives good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_norm = HistogramNormalisationLayer(\n",
    "    image_name='image',\n",
    "    modalities=['Modality0'],\n",
    "    model_filename=str(histogram_landmarks_path),\n",
    "    binary_masking_func=binary_masking_func,\n",
    "    cutoff=(0.001, 0.999),\n",
    "    name='hist_norm_layer',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set our image foreground to have zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitening = MeanVarNormalisationLayer(\n",
    "    image_name='image', binary_masking_func=binary_masking_func)\n",
    "preprocessing_layers = [\n",
    "    volume_padding_layer,\n",
    "    hist_norm,\n",
    "    whitening,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our preprocessed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ImageReader().initialise(data_parameters)\n",
    "reader.add_preprocessing_layers(preprocessing_layers)\n",
    "_, image_data_dict, _ = reader()\n",
    "preprocessed_image = image_data_dict['image']\n",
    "visualization.plot_volume(preprocessed_image, title='Preprocessed image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the effect of the whitening layer on the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_histogram(preprocessed_image, kde=False, ylim=(0, 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler and aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the whole image doesn't fit in most GPUs, we need to use a [patch-based](https://niftynet.readthedocs.io/en/dev/window_sizes.html) approach.\n",
    "\n",
    "<img src=\"https://github.com/fepegar/tf2pt/raw/master/images/patch.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NiftyNet's grid sampler to get all windows from the volume (blue in the previous image) and a grid samples aggregator (red) to reconstruct the output image from the inferred windows. If you have any memory issues, try reducing the window size.\n",
    "\n",
    "The [window border](https://niftynet.readthedocs.io/en/dev/window_sizes.html#border) is needed to reduce the border effect in a dense prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128\n",
    "window_size = 3 * (window_size, )\n",
    "window_border = 2, 2, 2\n",
    "window_size_dict = {'image': window_size}\n",
    "batch_size = 1\n",
    "\n",
    "sampler = GridSampler(\n",
    "    reader,\n",
    "    window_size_dict,\n",
    "    window_border=window_border,\n",
    ")\n",
    "\n",
    "prediction_pt_dir = tempdir / 'prediction'\n",
    "prediction_pt_dir.mkdir(exist_ok=True)\n",
    "aggregator = GridSamplesAggregator(\n",
    "    image_reader=reader,\n",
    "    window_border=window_border,\n",
    "    output_path=prediction_pt_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the most important part: run the parcellation! We'll iterate over the windows provided by the sampler, pass them through the network and aggregate them in the output volume (this might take a couple of minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "for batch_dict in sampler():\n",
    "    input_tensor = tf2pt.niftynet_batch_to_torch_tensor(batch_dict).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    labels = tf2pt.torch_logits_to_niftynet_labels(logits)\n",
    "    window_dict = dict(window=labels)\n",
    "    aggregator.decode_batch(window_dict, batch_dict['image_location'])\n",
    "\n",
    "# Release GPU memory\n",
    "del model\n",
    "del model_pretrained\n",
    "del input_tensor\n",
    "del logits\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's [run the inference using NiftyNet](https://github.com/NifTK/NiftyNetModelZoo/tree/5-reorganising-with-lfs/highres3dnet_brain_parcellation#generating-segmentations-for-example-data) as well, so that we can compare both results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!net_segment inference -c ~/niftynet/extensions/highres3dnet_brain_parcellation/highres3dnet_config_eval.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = utils.get_first_array(data_dir)\n",
    "labels_nn = utils.get_first_array(models_dir).astype(np.uint16)\n",
    "labels_pt = utils.get_first_array(prediction_pt_dir).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = labels_nn != labels_pt\n",
    "np.count_nonzero(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! ✨ Both parcellations are exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_path = 'GIFNiftyNet.ctbl'\n",
    "visualization.plot_volume(\n",
    "    labels_nn,\n",
    "    enhance=False,\n",
    "    colors_path=colors_path,\n",
    "    title='Parcellation inferred by NiftyNet',\n",
    ")\n",
    "visualization.plot_volume(\n",
    "    labels_pt,\n",
    "    enhance=False,\n",
    "    colors_path=colors_path,\n",
    "    title='Parcellation inferred by PyTorch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write this notebook I have used Ubuntu 18.04 installed on an Alienware 13 R3 laptop, which includes a 6 GB GeForce GTX 1060 NVIDIA GPU. I'm using CUDA 9.0.\n",
    "\n",
    "Inference using PyTorch took 5725 MB of GPU memory. TensorFlow usually takes as much as possible beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook locally, I recommend downloading the repository and creating a [`conda`](https://docs.conda.io/en/latest/miniconda.html) environment:\n",
    "\n",
    "```shell\n",
    "$ git clone https://github.com/fepegar/miccai-educational-challenge-2019.git\n",
    "$ cd miccai-educational-challenge-2019\n",
    "$ conda create -n emiccai python=3.6 -y\n",
    "$ conda activate emiccai && conda install jupyterlab -y && jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zenodo\n",
    "* conda\n",
    "* figshare\n",
    "* google colab\n",
    "* github\n",
    "* Docker"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MICCAI 2019.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
